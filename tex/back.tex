%\bibliographystyle{sjtu2}%[此处用于每章都生产参考文献]
\chapter{相关技术背景}
\label{chap:back}

\section{传统索引结构}

传统索引结构按照数据结构可以分为基于树结构的索引与基于哈希函数的索引，根据功能可以分为范围查询索引、点查询索引和存在索引。
{\li}受传统索引结构启发与影响，了解传统索引结构有助于更好理解{\li}与本文的研究内容。

\subsection{B树及其变种}

B树及其变种索引结构是最常被使用的索引结构之一\cite{graefe2001b}。
B树是一种平衡的固定扇出（fanout）的树状结构，对于指定扇出值$B$，
B树保证除了根节点（root node）外所有节点包含的数据量大于$B/2$并小于$B$，
从而保证B树的树高$H=O(log_B(n))$，其中$n$为数据量大小。
平衡二分搜索树（balanced binary search tree）可以看作是一种扇出值为$2$的B树特例。
为保证节点数据量要求，当某节点数据量大于$B$时，该节点将会分裂为两个，
当某节点数据小于$B/2$时，该节点将与邻居节点进行合并。
B树树高为$O(log_B(n))$的特性使其插入（insert）、删除（delete）、
修改（update）与查找（lookup）均能在对数时间内完成，在大数据量下具有良好的性能表现。
B树及其变种索引结构因此被广泛地使用在数据库、文件系统与操作系统中。

% 在计算机科学中，B树（英语：B-tree）是一种自平衡的树，能够保持数据有序。
% 这种数据结构能够让查找数据、顺序访问、插入数据及删除的动作，都在对数时间内完成。
% B树，概括来说是一个一般化的二叉查找树（binary search tree）一个节点可以拥有最少2个子节点。
% 与自平衡二叉查找树不同，B树适用于读写相对大的数据块的存储系统，例如磁盘。
% B树减少定位记录时所经历的中间过程，从而加快存取速度。
% B树这种数据结构可以用来描述外部存储。
% 这种数据结构常被应用在数据库和文件系统的实现上。

分形树[]（Fractal Tree）和B$^\epsilon$树[]（B$^\epsilon$-tree）是B树的一种变种，它们与B树类似，
将数据排序并通过固定大小的节点进行索引，从而提供对数时间的性能。然而它们每一个节点包含一个缓存（buffer），
允许将插入、删除等修改操作临时保存在中间节点。
通过使用缓存，这些修改只会在积累了一定数量后，才会被写入下一级节点，从而避免了B树在索引磁盘数据时，
任何大小的写操作都会导致一次费时的磁盘读写操作。
然而，这种将修改进行缓存并批量传播的方式同时引入了写放大（write amplification）问题，
即对实际发生在系统里的写操作数据量，包括缓存的修改与传播，远大于用户真实写入的数据量。
尽管如此，分形树被使用在商业数据库TokuDB[]中，同时也在一些文件系统学术原型中[]被使用。

% In computer science, a fractal tree index is a tree data structure that keeps data sorted and allows searches and sequential access in the same time as a B-tree but with insertions and deletions that are asymptotically faster than a B-tree.
%  Like a B-tree, a fractal tree index is a generalization of a binary search tree in that a node can have more than two children.
%  Furthermore, unlike a B-tree, a fractal tree index has buffers at each node, which allow insertions, deletions and other changes to be stored in intermediate locations.
%  The goal of the buffers is to schedule disk writes so that each write performs a large amount of useful work, thereby avoiding the worst-case performance of B-trees, in which each disk write may change a small amount of data on disk.
%  Like a B-tree, fractal tree indexes are optimized for systems that read and write large blocks of data.
%  The fractal tree index has been commercialized in databases by Tokutek.
%  Originally, it was implemented as a cache-oblivious lookahead array,[1] but the current implementation is an extension of the Bε tree.
% [2] The Bε is related to the Buffered Repository Tree.
% [3] The Buffered Repository Tree has degree 2, whereas the Bε tree has degree Bε.
%  The fractal tree index has also been used in a prototype filesystem.
% [4][5] An open source implementation of the fractal tree index is available,[6] which demonstrates the implementation details outlined below.

% uses a fraction of node storage to serve as an
% update buffer \cite{esmet2012tokufs, bender2015and}.
% The updates will be flush to children's buffer when current node's buffer is full and applied until they
% reach the leaf.
% This optimization aims to avoid frequent small writes to disk.
% However, propagating updates also introduce write amplification problem.

Masstree[]是一种结合字典树（trie）和B树的索引结构，它将键分割为许多长度为8字节（Byte）的段（segment）并用字典树进行索引。
因为每一个段的长度较长，一个段可以有大量不同的可能值（$2^32$，即$4294967296$），
因此每个字典树节点需要从大量的段值中快速查找出下一字典树节点的位置。
为解决单个字典树节点的查找性能问题，Masstree为每个字典树节点配备一个独立的B树。
当键的长度小于等于8字节时，Masstree等价于B树。
除此之外，Masstree是一个支持并发访问（concurrent access）的数据结构，它允许同时被多个线程（thread）发起读写操作。
在其内部实现中，Masstree使用乐观并发控制（optimistic concurrency control，OCC）保证对数据内容的读写原子性，
并使用细粒度锁机制（fine-grained locking）保证对元数据，包括内部节点，在更新操作下的一致性。

% \masst \cite{mao2012cache} partitions key into 8-byte segments and index them with a trie structure.
% Within each trie node, a concurrent \bt is used to index the segments.
% Similar to \sys, \masst uses optimistic concurrency control to ensure read/write atomicity and uses
% fine-grained locks to protect nodes during split and merge.

FITing-Tree[]是一个利用线性函数（piece-wise linear function）对B树操作时延（latency）与内存使用（memory consumption）进行优化的索引结构。
它通过使用线性函数拟合B树叶子节点（leaf node）中的数据分布，即键与位置的函数映射，达到节省内存空间的效果。
与B树不同之处在于，FITing-Tree并未限制叶子节点所包含的数据量，而是限制线性函数拟合结果的误差值（error）。
因为线性函数能够在给定的误差值范围内索引更多的数据，因此FITing-Tree的叶子节点远大于B树，从而降低中间节点的耗时，进而降低索引操作的时延。
相比于传统的节点结构，线性函数所需要的内存空间十分有限，从而节省大量的内存空间。
通过控制误差值，用户可以通过FITing-Tree的成本模型（cost model）预估其查询时延与内存使用情况，从而达到期望的性能表现。
然而，更新操作会使原有线性函数的误差值发生变化，违反用户指定的误差值，从而要求频繁对线性函数进行更新，造成索引性能下降。
为解决这一问题，FITing-Tree使用叶子结点内部缓存的方法，牺牲部分索引性能来降低对线性函数的更新频率。

% In this paper, we present a novel data-aware index structure called FITing-Tree which approximates an index using piece-wise linear functions with a bounded error specified at construction time.
% This error knob provides a tunable parameter that allows a DBA to FIT an index to a dataset and workload by being able to balance lookup performance and space consumption.
% To navigate this tradeoff, we provide a cost model that helps determine an appropriate error parameter given either (1) a lookup latency requirement (e.g., 500ns) or (2) a storage budget (e.g., 100MB).
% Using a variety of real-world datasets, we show that our index is able to provide performance that is comparable to full index structures while reducing the storage footprint by orders of magnitude.

Wormhole[]是一种结合B树、字典树与哈希表（hash table）的索引结构，它使用由哈希表编码的字典树代替B树的中间节点来索引叶节点，提供$O(log(L))$的查找时间，其中$L$为键的长度。
因为针对数据量较大时，B树高度以对数速度增长，然而字典树能够始终保持相对较为稳定的树高，因此Wormhole使用字典树来索引B树叶子节点的首键。
Wormhole进一步优化字典树性能，通过将字典树中每一个节点所代表的键前缀存入哈希表，从而字典树的查找可以通过在查询键上进行二分哈希表查找完成，从而实现$O(log(L))$的查找时间。
通过精巧设计与实现上的优化，Wormhole能够依旧保持较低的内存空间使用。

% In this paper we introduce a new ordered index structure, named Wormhole, that takes O(log L) worst-case time for looking up a key with a length of L.
%  The low cost is achieved by simultaneously leveraging strengths of three indexing structures, namely hash table, prefix tree, and B+ tree, to orchestrate a single fast ordered index.
%  Wormhole’s range operations can be performed by a linear scan of a list after an initial lookup.
%  This improvement of access efficiency does not come at a price of compromised space efficiency.
%  Instead, Wormhole’s index space is comparable to those of B+ tree and skip list.
%  Experiment results show that Wormhole outperforms skip list, B+ tree, ART, and Masstree by up to 8.4×, 4.9×, 4.3×, and 6.6× in terms of key lookup throughput, respectively.

\subsection{字典树及其变种}

字典树，也称作前缀树，是一种基于树状结构的索引结构。
相比于B树，字典树的高度是由被索引的键长决定的。
字典树将键分为多个等长的段，每一段对应字典树中的一层中间节点。
字典树的节点通过比较查找键的对应段的值，决定下一个节点的位置，因此它的节点不会像B树那样存放被索引的键的值。
因为字典树的高度取决于键长，为避免数据量较少时树高度过大影响性能，存在许多压缩字典树的研究预防法[]。

% In computer science, a trie, also called digital tree, radix tree or prefix tree, is a kind of search tree—an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings.
% Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated.
% All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string.
% Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest.
% Hence, keys are not necessarily associated with every node.
% For the space-optimized presentation of prefix tree, see compact prefix tree.

Height Optimized Trie[]（HOT）是一种基于字典树的空间高效索引结构。
HOT的思想在于动态的变化每个节点所需要的内存大小，达到稳定的高节点扇出值，从而优化字典树的高度与内存使用的效率。
通过将原有的字典树节点合并在一起，HOT使用合并节点（combined node）来保证高扇出的同时，整体树高保持在一个较低的水平。
节点的布局设计通过高效的工程实践达到紧凑的效果，并且允许使用单指令流多数据流（Single Instruction Multiple Data，SIMD）指令进行快速搜索。

% We present the Height Optimized Trie (HOT), a fast and spaceefficient in-memory index structure.
% The core algorithmic idea of HOT is to dynamically vary the number of bits considered at each node, which enables a consistently high fanout and thereby good cache efficiency.
% The layout of each node is carefully engineered for compactness and fast search using SIMD instructions.
% Our experimental results, which use a wide variety of workloads and data sets, show that HOT outperforms other state-of-the-art index structures for string keys both in terms of search performance and memory footprint, while being competitive for integer keys.
% We believe that these properties make HOT highly useful as a general-purpose index structure for main-memory databases.

Succinct Range Filter[]（SuRF）是一种简明数据结构\footnote{一种使用接近信息编码的下界的空间使用来存储数据的数据结构}（succinct data structure），它的核心是一个Fast Succinct Trie（FST）。
SuRF通过将储存的键通过简明编码的字典树进行索引，从而允许高效地达到范围查询的过滤器功能，即不用真实查询磁盘即知道范围内的键是否存在，有效减少范围查找对磁盘的访问开销。
SuRF基于以下观察：对于一棵字典树来说，上层节点较少但访问频繁，而下层节点则相对的访问较少缺占据较大空间。
因此，SuRF使用了两种数据结构来分别处理这两类节点，即热节点与冷节点。
在较上层使用了LOUDS-Dense的编码方式来存储节点，注重高效的查询效率。在较下层使用LOUDS-Sparse的编码方式来存储节点，注重空间的利用率。

% We present the Succinct Range Filter (SuRF), a fast and compact data structure for approximate membership tests.
% Unlike traditional Bloom filters, SuRF supports both single-key lookups and common range queries: open-range queries, closed-range queries, and range counts.
% SuRF is based on a new data structure called the Fast Succinct Trie (FST) that matches the point and range query performance of state-of-the-art order-preserving indexes, while consuming only 10 bits per trie node.
% The false positive rates in SuRF for both point and range queries are tunable to satisfy different application needs.
% We evaluate SuRF in RocksDB as a replacement for its Bloom filters to reduce I/O by filtering requests before they access on-disk data structures.
% Our experiments on a 100 GB dataset show that replacing RocksDB’s Bloom filters with SuRFs speeds up open-seek (without upper-bound) and closed-seek (with upper-bound) queries by up to 1.5× and 5× with a modest cost on the worst-case (all-missing) point query throughput due to slightly higher false positive rate.

\subsection{哈希表及其变种}

哈希表是一类利用哈希函数进行散列操作，提供常数时间查询性能的索引结构。
哈希函数对输入的键，计算一个索引位置，尽可能将原分布的键均匀分散在索引空间中。
理想情况下，哈希函数能够将键分散到唯一位置上，实际中哈希函数可能对不同的键输出同一索引位置，这种情况成为哈希冲突（hash collision）。
为解决哈希冲突的问题，常见的做法包括将冲突的键通过链表的方式存放在同一个索引位置下方，在查询时遍历所有匹配的键并比较真实键的值。
因此，哈希表的性能取决于哈希函数的计算速度以及其散列的能力。
此外，哈希表不支持范围查询。

% In computing, a hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values.
% A hash table uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.
% Ideally, the hash function will assign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash collisions where the hash function generates the same index for more than one key.
% Such collisions must be accommodated in some way.
% In a well-dimensioned hash table, the average cost (number of instructions) for each lookup is independent of the number of elements stored in the table.
% Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at (amortized[2]) constant average cost per operation.[3][4]
% In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure.
% For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.

Cuckoo Hash是简单的哈希表结构，能够提供最坏情况下常数时间的查询时间，接近于经典的完美哈希[]（perfect hashing）的理论性能。它的空间使用情况接近于二分搜索树，即平均每个键需要3个字长（word）。
Cuckoo Hash并未使用完美哈希算法，而是通过一种开放寻址（open addressing）的变种算法，允许键在探寻序列（probing sequence）中的移动来得到优异性能。
对于一个键，Cuckoo Hash通过计算两个不同的哈希函数来避免哈希冲突。
插入时，两个哈希函数的计算结果都可作为插入位置，若两个位置都被占用了，其中一个占用的键将会被移动到其另一个哈希函数的计算结果指定的位置。
这个过程将会重复直到没有冲突发生。

% We present a simple dictionary with worst case constant lookup time, equal- ing the theoretical performance of the classic dynamic perfect hashing scheme of Dietzfelbinger et al.
% The space usage is similar to that of binary search trees, i.e., three words per key on average.
% Besides being conceptually much simpler than previous dynamic dictionaries with worst case constant lookup time, our data structure is interesting in that it does not use perfect hashing, but rather a variant of open addressing where keys can be moved back in their probe sequences.
% An implementation inspired by our algorithm, but using weaker hash func- tions, is found to be quite practical.
% It is competitive with the best known dictionaries having an average case (but no nontrivial worst case) guarantee.

Level Hash是一个针对非易失性内存（Non-volatile memory，NVM）、对写优化的一种索引结构，弥补了传统索引结构受数据一致性（consistency）限制而产生的在持久性内存下性能下降的问题。
Level Hash提供一种基于共享的两级哈希表结构，实现最坏情况下常数时间的搜索、插入、删除和更行操作，并且极少触发昂贵的非易失性内存的额外写操作。
为了实现低开销的一致性保证，Level Hash对插入、删除和大小调整（resize）使用无日志（log-free）一致性方案，对更新使用乐观无日志一致性方案。
为了提供高性价比的大小调整操作，Level Hash通过原地（inplace）大小调整方案，只需对$1/3$的键进行重哈希（rehash）操作，从而大幅提高大小调整操作的性能。

% Non-volatile memory (NVM) as persistent memory is expected to substitute or complement DRAM in memory hierarchy, due to the strengths of non-volatility, high density, and near-zero standby power.
% However, due to the requirement of data consistency and hardware limita- tions of NVM, traditional indexing techniques originally designed for DRAM become inefficient in persistent memory.
% To efficiently index the data in persistent memory, this paper proposes a write-optimized and high-performance hashing index scheme, called level hashing, with low-overhead consistency guarantee and cost-efficient resizing.
% Level hashing provides a sharing- based two-level hash table, which achieves a constant- scale search/insertion/deletion/update time complexity in the worst case and rarely incurs extra NVM writes.
% To guarantee the consistency with low overhead, level hash- ing leverages log-free consistency schemes for insertion, deletion, and resizing operations, and an opportunistic log-free scheme for update operation.
% To cost-efficiently resize this hash table, level hashing leverages an in- place resizing scheme that only needs to rehash 1/3 of buckets instead of the entire table, thus significantly reducing the number of rehashed buckets and improving the resizing performance.
% Experimental results demon- strate that level hashing achieves 1.4×−3.0× speedup for insertions, 1.2×−2.1× speedup for updates, and over 4.3× speedup for resizing, while maintaining high search and deletion performance, compared with state- of-the-art hashing schemes.

\subsection{布隆过滤器}

Bloom filter

% 布隆过滤器（英语：Bloom Filter）是1970年由布隆提出的。
% 它实际上是一个很长的二进制向量和一系列随机映射函数。
% 布隆过滤器可以用于检索一个元素是否在一个集合中。
% 它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。

% 如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。
% 链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。
% 但是随着集合中元素的增加，我们需要的存储空间越来越大。
% 同时检索速度也越来越慢，上述三种结构的检索时间复杂度分别为O(n),O(log⁡n),O(1)。

% 布隆过滤器的原理是，当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。
% 检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。
% 这就是布隆过滤器的基本思想。

\section{机器学习}

\subsection{监督学习}

% Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.
% [1] It infers a function from labeled training data consisting of a set of training examples.
% [2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).
%  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples.
%  An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances.
%  This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias).

\subsection{回归问题}

% In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables.
% It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors').
% More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.
% Many techniques for carrying out regression analysis have been developed.

% Familiar methods such as linear regression and ordinary least squares regression are parametric, in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data.
% Nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions, which may be infinite-dimensional.

\subsection{自动机器学习}

% Machine learning techniques have deeply rooted in our everyday life.
% However, since it is knowledge- and labor-intensive to pursue good learning performance, humans are heavily involved in every aspect of machine learning.
% To make machine learning techniques easier to apply and reduce the demand for experienced human experts, automated machine learning (AutoML) has emerged as a hot topic with both industrial and academic interest.
% In this paper, we provide an up to date survey on AutoML.
% First, we introduce and define the AutoML problem, with inspiration from both realms of automation and machine learning.
% Then, we propose a general AutoML framework that not only covers most existing approaches to date, but also can guide the design for new methods.
% Subsequently, we categorize and review the existing works from two aspects, i.e., the problem setup and the employed techniques.
% The proposed framework and taxonomies provide a detailed analysis of AutoML approaches and explain the reasons underneath their successful applications.
% We hope this survey can serve as not only an insightful guideline for AutoML beginners but also an inspiration for future research.

\section{学习索引结构}

\subsection{递归模型索引}

% The insight is that indexes can
% be viewed as functions from the data (keys) to the values which represent either
% record positions in a sorted array (for range index), positions in an unsorted array
% (for Hash-Index) or whether the data exists or not (for BitMap-Index).
% For the case of range index, the function is effectively a cumulative distribution
% function (CDF). Given the CDF $F$, the positions can be predicted by:
%  $p=F(\text{Key}) \times N$, where $p$ is the position of the key and $N$ is the total number of keys.

% The core idea is to approximate the CDF function $F$ with machine learning models
% such as deep neural networks. While the choice of the model architectures
% can vary, the paper proposes a \emph{staged model} architecture inspired by
% the multi-stage structure of \bt. The sub-model at each internal stage predicts
% which sub-models to be activated in the next stage while the leaf stage
% directly predicts the CDF values. The models are trained from the root stage
% to the leaf stage, and each stage is trained separately using the following loss
% function:
% $L_l=\sum_{(x,y)}(f_l^{(\lfloor M_lf_{l-1}(x)/N\rfloor )}(x)-y)^2~;~L_0=\sum_{(x,y)}(f_0(x)-y)^2$, $(x,y)$ is the $(key, CDF\ value)$ pair from the dataset;
% $L_l$ is stage $l$'s loss function; $f_l^{(k)}$ is the $k^{th}$ sub-model of stage $l$; $M_l$ is the number of models at stage $l$.
% $f_{l-1}$ recursively executes above equation to the root
% stage $L_0$.

% To deploy the learned index, the approximation error needs to be corrected.
% The prediction error can be bounded by the maximal and minimal prediction error,
% $max\_err$ and $min\_err$,
% between the predicted and the actual positions for each key. Hence, if $pos$ is
% the predicted position by the learned index, the actual position should be
% within $[pos + min\_err, pos + max\_err]$, and a binary search can be used. The model error bound
% $log_2(max\_err - min\_err + 1)$, denoted as $e$, is thus a critical indicator of the effectiveness. It will be more effective with a smaller $e$.