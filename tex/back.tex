%\bibliographystyle{sjtu2}%[此处用于每章都生产参考文献]
\chapter{相关技术背景}
\label{chap:back}

本章将从介传统索引结构、机器学习以及{\li}三个方面介绍相关的背景知识，并且比较{\li}与传统索引结构在数据结构与算法上的差异。

\section{传统索引结构}

索引是一类用于提高数据查询效率的数据结构。
传统索引结构按照数据结构可以分为基于树结构的索引与基于哈希函数的索引，根据功能可以分为范围查询索引、点查询索引和存在索引。
{\li}受传统索引结构启发与影响，了解传统索引结构有助于更好理解{\li}与本文的研究内容。

\subsection{B树及其变种}

\begin{figure}[!htp]
  \centering
  \includegraphics{figure/btree.pdf}
  \bicaption[B树数据结构示意图]
    {B树数据结构示意图}
    {Illustration of B-tree data strucutre}
  \label{fig:btree}
\end{figure}

B树及其变种索引结构是最常被使用的索引结构之一\cite{graefe2001b}。
如图\ref{fig:btree}所示，B树是一种平衡的固定扇出（fanout）的树状结构，对于指定扇出值$B$，
B树保证除了根节点（root node）外所有节点包含的数据量大于$B/2$并小于$B$，
从而保证B树的树高$H=O(log_B(n))$，其中$n$为数据量大小。
平衡二分搜索树（balanced binary search tree）可以看作是一种扇出值为$2$的B树特例。
为保证节点数据量要求，当某节点数据量大于$B$时，该节点将会分裂为两个，
当某节点数据小于$B/2$时，该节点将与邻居节点进行合并。
B树树高为$O(log_B(n))$的特性使其插入（insert）、删除（delete）、
修改（update）与查找（lookup）均能在对数时间内完成，在大数据量下具有良好的性能表现。
B树及其变种索引结构因此被广泛地使用在数据库、文件系统与操作系统中。

% 在计算机科学中，B树（英语：B-tree）是一种自平衡的树，能够保持数据有序。
% 这种数据结构能够让查找数据、顺序访问、插入数据及删除的动作，都在对数时间内完成。
% B树，概括来说是一个一般化的二叉查找树（binary search tree）一个节点可以拥有最少2个子节点。
% 与自平衡二叉查找树不同，B树适用于读写相对大的数据块的存储系统，例如磁盘。
% B树减少定位记录时所经历的中间过程，从而加快存取速度。
% B树这种数据结构可以用来描述外部存储。
% 这种数据结构常被应用在数据库和文件系统的实现上。

分形树\cite{esmet2012tokufs}（Fractal Tree）和B$^\epsilon$树\cite{bender2015and}（B$^\epsilon$-tree）是B树的一种变种，它们与B树类似，
将数据排序并通过固定大小的节点进行索引，从而提供对数时间的性能。
然而它们每一个节点包含一个缓存（buffer），
允许将插入、删除等修改操作临时保存在中间节点。
通过使用缓存，这些修改只会在积累了一定数量后，才会被写入下一级节点，从而避免了B树在索引磁盘数据时，
任何大小的写操作都会导致一次费时的磁盘读写操作。
然而，这种将修改进行缓存并批量传播的方式同时引入了写放大（write amplification）问题，
即对实际发生在系统里的写操作数据量，包括缓存的修改与传播，远大于用户真实写入的数据量。
尽管如此，分形树被使用在商业数据库TokuDB中，同时也在一些文件系统学术原型中\cite{jannen2015betrfs, yuan2016optimizing, zhan2018full}被使用。

% In computer science, a fractal tree index is a tree data structure that keeps data sorted and allows searches and sequential access in the same time as a B-tree but with insertions and deletions that are asymptotically faster than a B-tree.
%  Like a B-tree, a fractal tree index is a generalization of a binary search tree in that a node can have more than two children.
%  Furthermore, unlike a B-tree, a fractal tree index has buffers at each node, which allow insertions, deletions and other changes to be stored in intermediate locations.
%  The goal of the buffers is to schedule disk writes so that each write performs a large amount of useful work, thereby avoiding the worst-case performance of B-trees, in which each disk write may change a small amount of data on disk.
%  Like a B-tree, fractal tree indexes are optimized for systems that read and write large blocks of data.
%  The fractal tree index has been commercialized in databases by Tokutek.
%  Originally, it was implemented as a cache-oblivious lookahead array,[1] but the current implementation is an extension of the Bε tree.
% [2] The Bε is related to the Buffered Repository Tree.
% [3] The Buffered Repository Tree has degree 2, whereas the Bε tree has degree Bε.
%  The fractal tree index has also been used in a prototype filesystem.
% [4][5] An open source implementation of the fractal tree index is available,[6] which demonstrates the implementation details outlined below.

% uses a fraction of node storage to serve as an
% update buffer \cite{esmet2012tokufs, bender2015and}.
% The updates will be flush to children's buffer when current node's buffer is full and applied until they
% reach the leaf.
% This optimization aims to avoid frequent small writes to disk.
% However, propagating updates also introduce write amplification problem.

Masstree\cite{mao2012cache}是一种结合字典树（trie）和B树的索引结构，它将键分割为许多长度为8字节（Byte）的段（segment）并用字典树进行索引。
因为每一个段的长度较长，一个段可以有大量不同的可能值（$2^32$，即$4294967296$），
因此每个字典树节点需要从大量的段值中快速查找出下一字典树节点的位置。
为解决单个字典树节点的查找性能问题，Masstree为每个字典树节点配备一个独立的B树。
当键的长度小于等于8字节时，Masstree等价于B树。
除此之外，Masstree是一个支持并发访问（concurrent access）的数据结构，它允许同时被多个线程（thread）发起读写操作。
在其内部实现中，Masstree使用乐观并发控制（optimistic concurrency control，OCC）保证对数据内容的读写原子性，
并使用细粒度锁机制（fine-grained locking）保证对元数据，包括内部节点，在更新操作下的一致性。

% \masst \cite{mao2012cache} partitions key into 8-byte segments and index them with a trie structure.
% Within each trie node, a concurrent \bt is used to index the segments.
% Similar to \sys, \masst uses optimistic concurrency control to ensure read/write atomicity and uses
% fine-grained locks to protect nodes during split and merge.

FITing-Tree\cite{fittingtree}是一个利用线性函数（piece-wise linear function）对B树操作时延（latency）与内存使用（memory consumption）进行优化的索引结构。
它通过使用线性函数拟合B树叶子节点（leaf node）中的数据分布，即键与位置的函数映射，达到节省内存空间的效果。
与B树不同之处在于，FITing-Tree并未限制叶子节点所包含的数据量，而是限制线性函数拟合结果的误差值（error）。
因为线性函数能够在给定的误差值范围内索引更多的数据，因此FITing-Tree的叶子节点远大于B树，从而降低中间节点的耗时，进而降低索引操作的时延。
相比于传统的节点结构，线性函数所需要的内存空间十分有限，从而节省大量的内存空间。
通过控制误差值，用户可以通过FITing-Tree的成本模型（cost model）预估其查询时延与内存使用情况，从而达到期望的性能表现。
然而，更新操作会使原有线性函数的误差值发生变化，违反用户指定的误差值，从而要求频繁对线性函数进行更新，造成索引性能下降。
为解决这一问题，FITing-Tree使用叶子结点内部缓存的方法，牺牲部分索引性能来降低对线性函数的更新频率。

% In this paper, we present a novel data-aware index structure called FITing-Tree which approximates an index using piece-wise linear functions with a bounded error specified at construction time.
% This error knob provides a tunable parameter that allows a DBA to FIT an index to a dataset and workload by being able to balance lookup performance and space consumption.
% To navigate this tradeoff, we provide a cost model that helps determine an appropriate error parameter given either (1) a lookup latency requirement (e.g., 500ns) or (2) a storage budget (e.g., 100MB).
% Using a variety of real-world datasets, we show that our index is able to provide performance that is comparable to full index structures while reducing the storage footprint by orders of magnitude.

Wormhole\cite{wu2019wormhole}是一种结合B树、字典树与哈希表（hash table）的索引结构，它使用由哈希表编码的字典树代替B树的中间节点来索引叶节点，提供$O(log(L))$的查找时间，其中$L$为键的长度。
因为针对数据量较大时，B树高度以对数速度增长，然而字典树能够始终保持相对较为稳定的树高，因此Wormhole使用字典树来索引B树叶子节点的首键。
Wormhole进一步优化字典树性能，通过将字典树中每一个节点所代表的键前缀存入哈希表，从而字典树的查找可以通过在查询键上进行二分哈希表查找完成，从而实现$O(log(L))$的查找时间。
通过精巧设计与实现上的优化，Wormhole能够依旧保持较低的内存空间使用。

% In this paper we introduce a new ordered index structure, named Wormhole, that takes O(log L) worst-case time for looking up a key with a length of L.
%  The low cost is achieved by simultaneously leveraging strengths of three indexing structures, namely hash table, prefix tree, and B+ tree, to orchestrate a single fast ordered index.
%  Wormhole’s range operations can be performed by a linear scan of a list after an initial lookup.
%  This improvement of access efficiency does not come at a price of compromised space efficiency.
%  Instead, Wormhole’s index space is comparable to those of B+ tree and skip list.
%  Experiment results show that Wormhole outperforms skip list, B+ tree, ART, and Masstree by up to 8.4×, 4.9×, 4.3×, and 6.6× in terms of key lookup throughput, respectively.

\subsection{字典树及其变种}

字典树，也称作前缀树，是一种基于树状结构的索引结构。
相比于B树，字典树的高度是由被索引的键长决定的。
字典树将键分为多个等长的段，每一段对应字典树中的一层中间节点。
字典树的节点通过比较查找键的对应段的值，决定下一个节点的位置，因此它的节点不会像B树那样存放被索引的键的值。
因为字典树的高度取决于键长，为避免数据量较少时树高度过大影响性能，存在许多压缩字典树的研究与方法\cite{morrison1968patricia, boehm2011efficient, leis2013adaptive}。

% In computer science, a trie, also called digital tree, radix tree or prefix tree, is a kind of search tree—an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings.
% Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key with which it is associated.
% All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string.
% Keys tend to be associated with leaves, though some inner nodes may correspond to keys of interest.
% Hence, keys are not necessarily associated with every node.
% For the space-optimized presentation of prefix tree, see compact prefix tree.

Height Optimized Trie\cite{binna2018hot}（HOT）是一种基于字典树的空间高效索引结构。
HOT的思想在于动态的变化每个节点所需要的内存大小，达到稳定的高节点扇出值，从而优化字典树的高度与内存使用的效率。
通过将原有的字典树节点合并在一起，HOT使用合并节点（combined node）来保证高扇出的同时，整体树高保持在一个较低的水平。
节点的布局设计通过高效的工程实践达到紧凑的效果，并且允许使用单指令流多数据流（Single Instruction Multiple Data，SIMD）指令进行快速搜索。

% We present the Height Optimized Trie (HOT), a fast and spaceefficient in-memory index structure.
% The core algorithmic idea of HOT is to dynamically vary the number of bits considered at each node, which enables a consistently high fanout and thereby good cache efficiency.
% The layout of each node is carefully engineered for compactness and fast search using SIMD instructions.
% Our experimental results, which use a wide variety of workloads and data sets, show that HOT outperforms other state-of-the-art index structures for string keys both in terms of search performance and memory footprint, while being competitive for integer keys.
% We believe that these properties make HOT highly useful as a general-purpose index structure for main-memory databases.

Succinct Range Filter\cite{zhang2018surf}（SuRF）是一种简明数据结构\footnote{一种使用接近信息编码的下界的空间使用来存储数据的数据结构}（succinct data structure），它的核心是一个Fast Succinct Trie（FST）。
SuRF通过将储存的键通过简明编码的字典树进行索引，从而允许高效地达到范围查询的过滤器功能，即不用真实查询磁盘即知道范围内的键是否存在，有效减少范围查找对磁盘的访问开销。
SuRF基于以下观察：对于一棵字典树来说，上层节点较少但访问频繁，而下层节点则相对的访问较少缺占据较大空间。
因此，SuRF使用了两种数据结构来分别处理这两类节点，即热节点与冷节点。
在较上层使用了LOUDS-Dense的编码方式来存储节点，注重高效的查询效率。
在较下层使用LOUDS-Sparse的编码方式来存储节点，注重空间的利用率。

% We present the Succinct Range Filter (SuRF), a fast and compact data structure for approximate membership tests.
% Unlike traditional Bloom filters, SuRF supports both single-key lookups and common range queries: open-range queries, closed-range queries, and range counts.
% SuRF is based on a new data structure called the Fast Succinct Trie (FST) that matches the point and range query performance of state-of-the-art order-preserving indexes, while consuming only 10 bits per trie node.
% The false positive rates in SuRF for both point and range queries are tunable to satisfy different application needs.
% We evaluate SuRF in RocksDB as a replacement for its Bloom filters to reduce I/O by filtering requests before they access on-disk data structures.
% Our experiments on a 100 GB dataset show that replacing RocksDB’s Bloom filters with SuRFs speeds up open-seek (without upper-bound) and closed-seek (with upper-bound) queries by up to 1.5× and 5× with a modest cost on the worst-case (all-missing) point query throughput due to slightly higher false positive rate.

\subsection{哈希表及其变种}

哈希表是一类利用哈希函数进行散列操作，提供常数时间查询性能的索引结构。
哈希函数对输入的键，计算一个索引位置，尽可能将原分布的键均匀分散在索引空间中。
理想情况下，哈希函数能够将键分散到唯一位置上，实际中哈希函数可能对不同的键输出同一索引位置，这种情况成为哈希冲突（hash collision）。
为解决哈希冲突的问题，常见的做法包括将冲突的键通过链表的方式存放在同一个索引位置下方，在查询时遍历所有匹配的键并比较真实键的值。
因此，哈希表的性能取决于哈希函数的计算速度以及其散列的能力。
此外，哈希表不支持范围查询。

% In computing, a hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values.
% A hash table uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.
% Ideally, the hash function will assign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash collisions where the hash function generates the same index for more than one key.
% Such collisions must be accommodated in some way.
% In a well-dimensioned hash table, the average cost (number of instructions) for each lookup is independent of the number of elements stored in the table.
% Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at (amortized[2]) constant average cost per operation.[3][4]
% In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure.
% For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.

Cuckoo Hash\cite{pagh2004cuckoo}是简单的哈希表结构，能够提供最坏情况下常数时间的查询时间，接近于经典的完美哈希\cite{botelho2005practical, lu2006perfect}（perfect hashing）的理论性能。
它的空间使用情况接近于二分搜索树，即平均每个键需要3个字长（word）。
Cuckoo Hash并未使用完美哈希算法，而是通过一种开放寻址（open addressing）的变种算法，允许键在探寻序列（probing sequence）中的移动来得到优异性能。
对于一个键，Cuckoo Hash通过计算两个不同的哈希函数来避免哈希冲突。
插入时，两个哈希函数的计算结果都可作为插入位置，若两个位置都被占用了，其中一个占用的键将会被移动到其另一个哈希函数的计算结果指定的位置。
这个过程将会重复直到没有冲突发生。

% We present a simple dictionary with worst case constant lookup time, equal- ing the theoretical performance of the classic dynamic perfect hashing scheme of Dietzfelbinger et al.
% The space usage is similar to that of binary search trees, i.e., three words per key on average.
% Besides being conceptually much simpler than previous dynamic dictionaries with worst case constant lookup time, our data structure is interesting in that it does not use perfect hashing, but rather a variant of open addressing where keys can be moved back in their probe sequences.
% An implementation inspired by our algorithm, but using weaker hash func- tions, is found to be quite practical.
% It is competitive with the best known dictionaries having an average case (but no nontrivial worst case) guarantee.

Level Hash\cite{zuo2018write}是一个针对非易失性内存（Non-volatile memory，NVM）、对写优化的一种索引结构，弥补了传统索引结构受数据一致性（consistency）限制而产生的在持久性内存下性能下降的问题。
Level Hash提供一种基于共享的两级哈希表结构，实现最坏情况下常数时间的搜索、插入、删除和更行操作，并且极少触发昂贵的非易失性内存的额外写操作。
为了实现低开销的一致性保证，Level Hash对插入、删除和大小调整（resize）使用无日志（log-free）一致性方案，对更新使用乐观无日志一致性方案。
为了提供高性价比的大小调整操作，Level Hash通过原地（inplace）大小调整方案，只需对$1/3$的键进行重哈希（rehash）操作，从而大幅提高大小调整操作的性能。

% Non-volatile memory (NVM) as persistent memory is expected to substitute or complement DRAM in memory hierarchy, due to the strengths of non-volatility, high density, and near-zero standby power.
% However, due to the requirement of data consistency and hardware limita- tions of NVM, traditional indexing techniques originally designed for DRAM become inefficient in persistent memory.
% To efficiently index the data in persistent memory, this paper proposes a write-optimized and high-performance hashing index scheme, called level hashing, with low-overhead consistency guarantee and cost-efficient resizing.
% Level hashing provides a sharing- based two-level hash table, which achieves a constant- scale search/insertion/deletion/update time complexity in the worst case and rarely incurs extra NVM writes.
% To guarantee the consistency with low overhead, level hash- ing leverages log-free consistency schemes for insertion, deletion, and resizing operations, and an opportunistic log-free scheme for update operation.
% To cost-efficiently resize this hash table, level hashing leverages an in- place resizing scheme that only needs to rehash 1/3 of buckets instead of the entire table, thus significantly reducing the number of rehashed buckets and improving the resizing performance.
% Experimental results demon- strate that level hashing achieves 1.4×−3.0× speedup for insertions, 1.2×−2.1× speedup for updates, and over 4.3× speedup for resizing, while maintaining high search and deletion performance, compared with state- of-the-art hashing schemes.

\subsection{布隆过滤器}

布隆过滤器是一个用来判断一个键是否存在的索引结构，对于存在的键它可以给出正确的判断，而对于不存在的键它有一定概率会给出误判，即假阳性（false positive）结果。
布隆过滤器基于一系列哈希函数与一个连续数组实现。
对于存在的键，它将每个哈希函数计算结果对应位置的数组内容置为1。
当查询一个键时，若它的哈希函数计算结果对应位置中存在一个或一个以上的非1值（0），则表明查询键不存在，反之则认为查询键可能存在。
布隆过滤器可以通过调整连续数组的大小来控制误判率，这给使用者平衡空间使用与误判率的一个调整参数。

% 布隆过滤器（英语：Bloom Filter）是1970年由布隆提出的。
% 它实际上是一个很长的二进制向量和一系列随机映射函数。
% 布隆过滤器可以用于检索一个元素是否在一个集合中。
% 它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。

% 如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。
% 链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。
% 但是随着集合中元素的增加，我们需要的存储空间越来越大。
% 同时检索速度也越来越慢，上述三种结构的检索时间复杂度分别为O(n),O(log⁡n),O(1)。

% 布隆过滤器的原理是，当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。
% 检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。
% 这就是布隆过滤器的基本思想。

\section{机器学习}

机器学习研究如何通过算法与统计学模型使计算机系统基于对模式的观察与推断可以不需要人为提供指示而可以有效地进行某项具体任务。
其中，深度学习（Deep Learning）使目前热门的一项机器学习研究领域，它们关注如何使用深度神经网络（Deep Neural Network，DNN）完成特定的任务。
{\li}通过使用{\model}与算法构建了高效的索引结构。

% Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead.
% It is seen as a subset of artificial intelligence.
% Machine learning algorithms build a mathematical model based on sample data, known as "training data", in order to make predictions or decisions without being explicitly programmed to perform the task.
% [1][2]:2 Machine learning algorithms are used in a wide variety of applications, such as email filtering, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task.
% Machine learning is closely related to computational statistics, which focuses on making predictions using computers.
% The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning.
% Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.[3][4] In its application across business problems, machine learning is also referred to as predictive analytics.

\subsection{监督学习}

监督学习使一类学习从输入到输出函数映射的机器学习任务，它会显式地提供被标注（labeled）的训练数据\cite{chapelle2009semi, moller1993scaled, zhu2005semi}。
在监督学习中，每一条训练数据都包括输入值和期望的输出值。
监督学习算法通过对训练数据的分析来获得推断函数，这个函数将会被用来映射新的数据。
理想情况下，推断函数能够正确地对未曾见过的输入值的输出值，然而实际中推断函数往往存在误差值。
因此，监督学习中学习算法需要具有良好的泛化能力\cite{kawaguchi2017generalization, neyshabur2017exploring, zhang2016understanding}（generalizability）。
与之相对的非监督学习则是通过间接的反馈，而非直接的函数映射数据进行{\model}训练\cite{ghahramani2003unsupervised, dayan1999unsupervised, sathya2013comparison}。

% Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.
% [1] It infers a function from labeled training data consisting of a set of training examples.
% [2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).
%  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples.
%  An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances.
%  This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias).

% \subsection{回归问题}

% In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables.
% It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors').
% More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.
% Many techniques for carrying out regression analysis have been developed.

% Familiar methods such as linear regression and ordinary least squares regression are parametric, in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data.
% Nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions, which may be infinite-dimensional.

\subsection{自动机器学习}

自动机器学习（Automated machine learning，AutoML）旨在端到端（end-to-end）的自动化将机器学习应用在真实场景问题上的流程，是目前热门的研究领域\cite{feurer2015efficient, thornton2013auto, kotthoff2017auto}。
在典型的机器学习应用中，使用者需要手动地进行数据预处理（data pre-processing）、特征工程（feature engineering）、特称提取（feature extraction）等工作\cite{heaton2016empirical, nargesian2017learning}。
此外，他们还需要选择恰当地算法并对超参数（hyperparameter）进行优化来获得最好的{\model}预测性能\cite{snoek2012practical, brochu2010tutorial}。
这些工作为机器学习的应用带来了巨大的挑赞。
自动机器学习通过基于机器学习本身的方法，来解决上述应用机器学习的挑战。
目前，在特定任务上，自动机器学习可以给出接近甚至超过人工设计的机器学习方案。
然而，自动机器学习也对训练数据、训练方法以及{\model}选择本身提出了挑战。

% Automated machine learning (AutoML) is the process of automating the end-to-end process of applying machine learning to real-world problems.
% In a typical machine learning application, practitioners must apply the appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods that make the dataset amenable for machine learning.
% Following those preprocessing steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their final machine learning model.
% As many of these steps are often beyond the abilities of non-experts, AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning.
% [1][2] Automating the end-to-end process of applying machine learning offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform models that were designed by hand.


% Machine learning techniques have deeply rooted in our everyday life.
% However, since it is knowledge- and labor-intensive to pursue good learning performance, humans are heavily involved in every aspect of machine learning.
% To make machine learning techniques easier to apply and reduce the demand for experienced human experts, automated machine learning (AutoML) has emerged as a hot topic with both industrial and academic interest.
% In this paper, we provide an up to date survey on AutoML.
% First, we introduce and define the AutoML problem, with inspiration from both realms of automation and machine learning.
% Then, we propose a general AutoML framework that not only covers most existing approaches to date, but also can guide the design for new methods.
% Subsequently, we categorize and review the existing works from two aspects, i.e., the problem setup and the employed techniques.
% The proposed framework and taxonomies provide a detailed analysis of AutoML approaches and explain the reasons underneath their successful applications.
% We hope this survey can serve as not only an insightful guideline for AutoML beginners but also an inspiration for future research.

\section{\li}

{\li}提出了使用{\model}完成索引工作的图景，并提出一系列使用{\model}的索引结构设计初步地评估了{\li}地有效性。
{\li}将索引视为从键到值——如在有序数组中的数据位置，在无序数据中的位置，抑或是表示数据是否存在的布尔值（boolean value）——的一个函数映射。
{\li}实际上所做的即是高效地学习所需要的函数映射。
论文提出了三类学习索引{------}有序索引、无序索引和存在索引，
其中有序索引与无序索引使用相同的架构设计，{\rmi}，而存在索引则是基于循环神经网络（Recurrent Neural Network，RNN）与卷积神经网络（Convolutional Neural Network，CNN）。

\subsection{{\rmi}}
\label{sec:rmi}

直接使用{\lr}、{\nn}或者其他{\model}来学习数据分布的位置充满挑战，因为使用一个{\model}将从数百万条数据的量级将误差降低到几百的水平是非常困难的。
但同时，使用一个{\model}将从数百万条数据的量级将误差降低到几万的水平，比如使用{\model}来代替B树最根部的两层，却很容易做到。
类似地，使用一个{\model}将从数万条数据的量级将误差降低到几百的水平也是一个相对简单的任务，因为{\model}只需要关注一部分数据。

% As outlined in Section 2.3 one of the key challenges of building alternative learned models to replace B-Trees is the accuracy for last-mile search.
% For example, reducing the prediction error to the order of hundreds from 100M records using a single model is often difficult.
% At the same time, reducing the error to 10k from 100M, e.g., a precision gain of 100 ∗ 100 = 10000 to replace the first 2 layers of a B-Tree through a model, is much easier to achieve even with simple models.
% Similarly, reducing the error from 10k to 100 is a simpler problem as the model can focus only on a subset of the data.

\begin{figure}[!htp]
  \centering
  \includegraphics{figure/rmi.pdf}
  \bicaption[{\rmi}数据结构示意图]
    {{\rmi}数据结构示意图}
    {Illustration of RMI data strucutre}
  \label{fig:rmi}
\end{figure}

因此{\li}提出一种递归的回归架构，{\rmi}，如图\ref{fig:rmi}所示。
通过构建一个{\model}的层级架构，每一级的{\model}以搜索键为输入，并挑选一个下一级所用的{\model}，直到最后一级的{\model}对该搜索键对应的数据位置进行预测。

% Based on that observation and inspired by the mixture of experts work [62], we propose the recursive regression model (see Figure 3).
% That is, we build a hierarchy of models, where at each stage the model takes the key as an input and based on it picks another model, until the final stage predicts the position.
% More formally, for our model f (x) where x is the key and y ∈ [0, N) the position, we assume at stage  there are M  models.
% We train the model at stage 0, f 0 (x) ≈ y.
% As such, (k) model k in stage , denoted by f  , is trained with loss:
% Note, we use here the notation of f −1 (x) recursively exe(M f (x)/N ) cuting f −1 (x) = f −1 −1 −2 (x).
% In total, we iteratively
% train each stage with loss L  to build the complete model.

对于范围索引，所需要的函数映射实际上是键的{\cdf}（Cumulative Distribution Function，CDF）。
给定键$Key$和{\cdf}$f$，数据的位置可以通过对{\cdf}放大$N$倍得到，其中$N$是被索引的键的总数。
\[ pos=f(Key) \times N \]

% The insight is that indexes can
% be viewed as functions from the data (keys) to the values which represent either
% record positions in a sorted array (for range index), positions in an unsorted array
% (for Hash-Index) or whether the data exists or not (for BitMap-Index).
% For the case of range index, the function is effectively a cumulative distribution
% function (CDF).
% Given the CDF $F$, the positions can be predicted by:
%  $p=F(\text{Key}) \times N$, where $p$ is the position of the key and $N$ is the total number of keys.

{\rmi}的核心想法使通过层级化的{\model}来学习{\cdf}$f$，比如{\lr}或{\nn}，得到近似的{\cdf}$f'$。
% 尽管可以使用各种{\model}来完成这一任务，受B树结构启发，原论文提出一种多级的{\model}架构，除第一级外每级包含多个{\model}。
% 如图\ref{rmi}所示，在中间的级，通过{\model}学习的{\cdf}预测结果来决定下一级使用的{\model}，而在最后一级的{\model}的预测结构将作为对键{\cdf}值的最终预测。
每一个{\model}都依据以下损失函数（loss function）进行训练{------}
\[ L_l=\sum_{(x,y)}(f_l^{(\lfloor M_lf_{l-1}(x)/N\rfloor )}(x)-y)^2 \]
\[ L_0=\sum_{(x,y)}(f_0(x)-y)^2 \]
，其中$(x,y)$是来自训练集的<键，{\cdf}值>对，
$L_l$是第$l$级的损失函数，$f_l^{(k)}$是第$l$级的第$k$个子{\model}，$M_l$是第$l$级的子{\model}数量。
$f_l$会递归地执行这一方程直到根级$L_0$。
因为数据的分配有中间级{\model}决定，从根到叶子级每个{\model}所处理的数据不断减少，从而提高{\model}预测的准确度。
一种看待不同{\model}的视角是每个{\model}对给定搜索键进行带有一定误差值的预测，这一预测结果将被用来选择下一级所用到的{\model}，下一级的{\model}因此只需要负责一部分键的范围的数据，
因此可以拥有较小的误差值。

% The core idea is to approximate the CDF function $F$ with machine learning models
% such as deep neural networks.
% While the choice of the model architectures
% can vary, the paper proposes a \emph{staged model} architecture inspired by
% the multi-stage structure of \bt.
% The sub-model at each internal stage predicts
% which sub-models to be activated in the next stage while the leaf stage
% directly predicts the CDF values.
% The models are trained from the root stage
% to the leaf stage, and each stage is trained separately using the following loss
% function:
% $L_l=\sum_{(x,y)}(f_l^{(\lfloor M_lf_{l-1}(x)/N\rfloor )}(x)-y)^2~;~L_0=\sum_{(x,y)}(f_0(x)-y)^2$, $(x,y)$ is the $(key, CDF\ value)$ pair from the dataset;
% $L_l$ is stage $l$'s loss function; $f_l^{(k)}$ is the $k^{th}$ sub-model of stage $l$; $M_l$ is the number of models at stage $l$.
% $f_{l-1}$ recursively executes above equation to the root
% stage $L_0$.
% One way to think about the different models is that each model makes a prediction with a certain error about the position for the key and that the prediction is used to select the next model, which is responsible for a certain area of the keyspace to make a better prediction with a lower error.

尽管如此，{\rmi}并不是一颗树。
正如图\ref{fig:rmi}所示，很可能不同的{\model}会选择同一个下一级的{\model}。
除此之外，每个{\model}也不一定会像B树一样包含相同数量的数据，尽管按照这个算法，如果存在无误差的完美{\model}，每个{\model}将会被分到相同数量的数据。
最后，根据不用的{\model}选择，中间级的选择不一定需要被当作是数据位置或者{\cdf}的预测，而更应该被考虑为是选择了一个对该键有更多信息，可以做更好预测的“专家”，即选择的下一级{\model}。

% However, recursive model indexes do not have to be trees.
% As shown in Figure 3 it is possible that different models of one stage pick the same models at the stage below.
% Furthermore, each model does not necessarily cover the same amount of records like B-Trees do (i.e., a B-Tree with a page-size of 100 covers 100 or less records).
% 4 Finally, depending on the used models the predictions between the different stages can not necessarily be interpreted as positions estimates, rather should be considered as picking an expert which has a better knowledge about certain keys (see also [62]).

这样的架构有如下优势：（1）它从执行代价中分离了{\model}的大小和复杂读；（2）它利用了学习{\cdf}的大致形状是一件较为容易的任务这一事实；（3）它有效地将数据分为了小的子范围，类似于B树将最后的精确定位的代价减少到了只需要非常少的操作即可完成；
以及（4）在级与级之间不存在搜索定位的过程，因为我们直接使用上级{\model}的预测结果作为索引进行选取下一级的{\model}。

% This model architecture has several benefits: (1) It separates model size and complexity from execution cost.
% (2) It leverages the fact that it is easy to learn the overall shape of the data distribution.
% (3) It effectively divides the space into smaller subranges, like a B-Tree, to make it easier to achieve the required “last mile” accuracy with fewer operations.
% (4) There is no search process required in-between the stages.
% For example, the output of Model 1.1 is directly used to pick the model in the next stage.
% This not only reduces the number of instructions to manage the structure, but also allows representing the entire index as a sparse matrix-multiplication for a TPU/GPU.

\subsection{{\rmi}训练与推断}
\label{sec:rmi-train-inference}

\begin{algorithm}
% \begin{algorithm}[H] % 强制定位
\caption{{\rmi}训练}
\label{algo:rmi-train}
\begin{algorithmic}[1] %每行显示行号
\Require 整形数组$stages$，数据数组$data$，{\model}信息 % 输入
\Ensure 训练完成的{\rmi}（{\model}数组$models$） % 输出
\State $M \gets stages.length$
\label{rmi-train:line:stage-length}
\State $models[][]$
\label{rmi-train:line:init-models}
\State $tmp\_records[][]$
\label{rmi-train:line:init-data}
\State $tmp\_records[1][1] \gets data$
\label{rmi-train:line:init-data-root}
\For{$i = 1 \to M$}
  \For{$j = 1 \to stages[i]$}
  \label{rmi-train:line:stage-begin}
    \State $models[i][j] \gets model~trained~with~tmp\_records[i][j]$
    \label{rmi-train:line:train-model}
    \If{$i < M$}
    \label{rmi-train:line:assign-precond}
      \For{$rec \in tmp\_records[i][j]$}
      \label{rmi-train:line:assign-begin}
        \State $p \gets models[i][j].predict(rec) / stages[i+1]$
        \State $tmp\_records[i+1][p].add(rec)$
      \EndFor
      \label{rmi-train:line:assign-end}
    \EndIf
  \EndFor
  \label{rmi-train:line:stage-end}
\EndFor
\For{$j = 1 \to models[M].length$}
\label{rmi-train:line:error-begin}
  \State $models[M][j].calculate\_error(tmp\_records[M][j])$
\EndFor
\label{rmi-train:line:error-end}
\State \Return $models$
\label{rmi-train:line:return}
\end{algorithmic}
\end{algorithm}

算法\ref{algo:rmi-train}是{\rmi}的训练算法。
它的输入如下：
\begin{itemize}
  \item \textbf{整形数组$stages$}，即由{\rmi}每一级所用{\model}个数组成的数组。
  注意，此数组的第一个元素必须为1，因为{\rmi}的第一级只允许拥有一个{\model}，否则无法保证确定性的索引执行路径；
  \item \textbf{数据数组$data$}，即被{\rmi}所索引的真实数据，它由所有键所对应的<键，{\cdf}值>对组成；
  \item \textbf{{\model}信息}，即每一级的{\model}相关的配置信息，它包含{\model}类型，以及各类型{\model}特定的信息，如对于{\nn}而言，它还包含每个{\nn}所使用的隐藏层的宽度以及隐藏层的个数。
\end{itemize}
它的输入为训练完成的{\rmi}（{\model}数组$models$）。

训练过程是每一级自上而下分别进行的，即上一级所有{\model}训练完成后才进行下一级{\model}的训练。
训练开始，首先将{\model}层级个数记录在变量$M$中，如第\ref{rmi-train:line:stage-length}所示，并初始化{\rmi}每一级的{\model}，用二维数组$models$进行存储，如第\ref{rmi-train:line:init-models}行所示，
最后初始化临时训练数据存储容器$tmp\_records$，这也是一个二维数组，如第\ref{rmi-train:line:init-data}。
因为接下来所有的数据需要由第一级的唯一{\model}进行分配，
因此该{\model}需要使用所有的数据进行训练，所以在进入分级训练前，在第\ref{rmi-train:line:init-data-root}行我们将所有数据分配给了这个{\model}，即$tmp\_records[1][1]$。
注意，在算法中$models$与$tmp\_records$的第一个下标均表示对应{\model}或训练数据所属的层级编号，第二个下标表示对应{\model}或训练数据所属该层级中的{\model}编号。
第\ref{rmi-train:line:stage-begin}行到第\ref{rmi-train:line:stage-end}行展示了对一个级里的{\model}训练的过程。
在本算法中我们略去了{\model}训练过程，而用第\ref{rmi-train:line:train-model}行简单表示，这是因为不同的{\model}的训练方式各不相同，同一种{\model}也可以通过不同的优化算法进行训练，
这些细节上的区别与{\rmi}的设计与实现都是正交的。
若第$i$级不是最后一级（第\ref{rmi-train:line:assign-precond}），在训练第$i+1$级{\model}前，如
第\ref{rmi-train:line:assign-begin}行到第\ref{rmi-train:line:assign-end}行所示，由刚完成训练的{\model}$models[i][j]$会将其负责的数据再次根据该{\model}的预测（训练）结果计算出下一级所应该选择的{\model}编号$p$，
并将对应的数据添加到该下一级{\model}所对应的训练数据中，即$tmp\_records[i+1][p]$。
当最后一级{\model}完成训练后，{\rmi}的训练也随之完成。
在返回前，我们需要计算每个最后级{\model}的误差值，已用于{\rmi}推断，如第\ref{rmi-train:line:error-begin}行到第\ref{rmi-train:line:error-end}行所示。
第\ref{rmi-train:line:return}行将返回通过逐级训练获得的{\model}结果，即训练后的{\model}参数。

\begin{algorithm}
% \begin{algorithm}[H] % 强制定位
\caption{{\rmi}推断}
\label{algo:rmi-inference}
\begin{algorithmic}[1] %每行显示行号
\Require 训练完成的{\rmi}（{\model}数组$models$），查询键$key$ % 输入
\Ensure 预测的数据位置$pos$，预测的最大误差$max\_err$，预测的最小误差$min\_err$ % 输出
\State $M \gets models.length$
\label{rmi-inference:line:init-depth}
\State $pred \gets null$
\label{rmi-inference:line:init-pred}
\State $p \gets 1$
\label{rmi-inference:line:init-p}
\For{$i = 1 \to M$}
\label{rmi-inference:line:begin}
  \State $pred = models[i][p].predict(key)$
  \State $p = pred / models[i+1].length$
\EndFor
\label{rmi-inference:line:end}
\State \Return $pred, models[M][p].max\_err, models[M][p].min\_err$
\label{rmi-inference:line:return}
\end{algorithmic}
\end{algorithm}

算法\ref{algo:rmi-train}是{\rmi}的推断算法。
它的输入如下：
\begin{itemize}
  \item \textbf{训练完成的{\rmi}（{\model}数组$models$）}，即由{\model}组成的二维数组，其第一个下标表示对应{\model}所属的层级编号，第二个下标表示对应{\model}所属该层级中的{\model}编号；
  \item \textbf{查询键$key$}，即使用{\rmi}进行查询的键。
\end{itemize}
它的输出如下：
\begin{itemize}
  \item \textbf{预测的数据位置$pos$}，即{\rmi}对输入查询键$key$对应数据位置的预测结果，该键必须是在训练是存在的键，否则预测结果将不准确；
  \item \textbf{预测的最大误差$max\_err$}，即{\rmi}对输入查询键$key$预测误差的上界；
  \item \textbf{预测的最小误差$min\_err$}，即{\rmi}对输入查询键$key$预测误差的下界。
\end{itemize}

与{\rmi}训练算法类似，{\rmi}推断算法也是一个自上而下逐级计算的算法。
在推断过程的开始，算法首先从{\model}数据中获得该{\rmi}所包含的层级数$M$，如第\ref{rmi-inference:line:init-depth}行所示。
随后初始化预测结果变量$pred$，如第\ref{rmi-inference:line:init-pred}行所示，以及下一级{\model}编号变量$p$，如第\ref{rmi-inference:line:init-p}行所示。
这两个变量将在递归的推断过程中被使用并改变。
第\ref{rmi-inference:line:begin}行到第\ref{rmi-inference:line:end}行展示了这一递归推断过程。
在每一级中，算法只使用一个{\model}进行预测，并根据该{\model}的预测结果更新下一级使用的{\model}的编号。
在进入这一递归过程之前，算法指定第一级使用的{\model}编号为1，因为第一级仅包含这一个{\model}，从而保证{\rmi}的推断具有一个共同的开始点。
经过最后一级的{\model}预测之后，数据位置的预测值与最后一级所使用的{\model}编号已经分别保存在变量$pred$和$p$中。
这时，预测结果与对应的{\model}的最大误差、最小误差，即$models[M][p].max\_err$、$models[M][p].min\_err$，一并返回给用户，
如第\ref{rmi-inference:line:return}行所示。

为了使用{\rmi}，预测误差值需要被纠正。
事实上，预测误差值可以通过最大误差$max\_err$和最小误差值$min\_err$进行约束。
\[ max\_err = \max_{key\ in\ training\ set}(f'(key)-f(key)) \]
\[ min\_err = \min_{key\ in\ training\ set}(f'(key)-f(key)) \]
因此，对于{\rmi}预测的数据位置$pos$，真实的数据位置应该在$[pos + min\_err, pos + max\_err]$范围内，
这是可以通过二分查找的方法进行数据的精确定位。
正因为如此，{\model}误差范围
$$e = log_2(max\_err - min\_err + 1)$$
成为表示{\rmi}有效性的关键指标。
越小的{\model}误差将会带来越高效的{\rmi}。

% To deploy the learned index, the approximation error needs to be corrected.
% The prediction error can be bounded by the maximal and minimal prediction error,
% $max\_err$ and $min\_err$,
% between the predicted and the actual positions for each key.
% Hence, if $pos$ is
% the predicted position by the learned index, the actual position should be
% within $[pos + min\_err, pos + max\_err]$, and a binary search can be used.
% The model error bound
% $log_2(max\_err - min\_err + 1)$, denoted as $e$, is thus a critical indicator of the effectiveness.
% It will be more effective with a smaller $e$.

\subsection{{\li}实例}

对于有序索引，{\li}直接使用上述{\rmi}对{\cdf}函数进行学习，并通过计算最大误差和最小误差的方法完成数据的精确定位。

对于无序索引，{\li}也可以利用数据分布学习到更好的哈希函数。
与有序索引相反，无序索引不需要将数据按照严格的排序关系紧凑地存储在一起。
{\li}将学习的{\cdf}函数放大$M$倍作为哈希函数，其中$M$是哈希表的目标大小。
\[ h(key) = f'(key) \times M \]
如果{\rmi}能够完美地学习真实{\cdf}$f$，以上哈希函数将不会产生任何冲突（当$M$大于索引数据量时）。
除此之外，这一个学习到的哈希函数可以应用在各种哈希表架构下，因为它并没有对无序索引的实现有任何假设。

% Surprisingly, learning the CDF of the key distribution is one potential way to learn a better hash function.
% However, in contrast to range indexes, we do not aim to store the records compactly or in strictly sorted order.
% Rather we can scale the CDF by the targeted size M of the Hash-map and use h(K) = F(K)∗M, with key K as our hash-function.
% If the model F perfectly learned the empirical CDF of the keys, no conflicts would exist.
% Furthermore, the hash-function is orthogonal to the actual Hash-map architecture and can be combined with separate chaining or any other Hash-map type.

% For the model, we can again leverage the recursive model architecture from the previous section.
% Obviously, like before, there exists a trade-off between the size of the index and performance, which is influenced by the model and dataset.

存在索引可以通过两种方案在{\li}中实现。
一种方法是将存在索引视为二分概率分类任务（binary probabilistic classification task）。
通过{\model}来预测查询键$key$是否是一个存在的键。
比如说，对于字符串，{\li}可以使用循环神经网络与卷积神经网络来完成这一分类任务。
学习过程中需要提供不存在的键已告知{\model}有关不存在的键的相关信息。
% One way to frame the existence index is as a binary probabilistic classification task.
% That is, we want to learn a model f that can predict if a query x is a key or non-key.
% For example, for strings we can train a recurrent neural network (RNN) or convolutional neural network (CNN) [37, 64] with D = {(x i ,y i = 1)|x i ∈ K} ∪ {(x i ,y i = 0)|x i ∈ U}.
% Because this is a binary classification task, our neural network has a sigmoid activation to produce a probability and is trained to minimize the log loss:  L = (x,y)∈D y log f (x) + (1 − y) log(1 − f (x)).

另一种方式是通过学习一种哈希函数来最大化存在的键之间的冲突，最大化不存在的键之间的冲突，并最小化存在的键与不存在的键的冲突。
这种方法可以视为是对布隆过滤器的优化，因为它仅仅替换了布隆过滤器中的哈希函数，而仍然依赖布隆过滤器的算法来完成存在索引任务。
有趣的是，这一哈希函数同样可以通过循环神经网络与卷积神经网络进行学习。

% An alternative approach to building existence indexes is to learn a hash function with the goal to maximize collisions among keys and among non-keys while minimizing collisions of keys and non-keys.
% Interestingly, we can use the same probabilistic classification model as before to achieve that.
% That is, we can create a hash function d, which maps f to a bit array of size m by scaling its output as d = f (x) ∗ m As such, we can use d as a hash function just like any other in a Bloom filter.

本文主要探讨{\li}的范围索引结构，在接下来的篇幅中，我们将混合地使用{\li}与{\rmi}这两个属于来指代{\li}的范围索引结构。

\section{本章小结}
本章介绍了传统索引结构、机器学习以及{\li}的相关背景知识，比较了{\li}与传统索引结构在数据结构与算法上的差异。
相比传统索引结构，{\li}通过{\model}进行对数据位置进行预测，通过记录预测的误差来提供一个可靠的搜索范围。
因为{\model}能够高效地减小搜索误差，所以{\li}查询的执行时间较短，空间消耗较低。
